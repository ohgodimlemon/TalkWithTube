# 🎥 TalkWithTube – Your Video Chatbot  

TalkWithTube is an **AI-powered chatbot for YouTube videos**. Leveraging **RAG-Retrieval Augmented Generation**, it allows you to interact with videos like never before — ask apecific questions, get summaries, and extract key insights instantly without scrubbing through hours of content.  

---

## ✨ Overview  

- **Problem**: Long lectures, podcasts, and tutorials make it hard to quickly find the exact information you need. Scrubbing through timelines wastes hours.  
- **Solution**: TalkWithTube leverages AI to let you *chat with videos*. Enter a YouTube link, and ask anything. The chatbot pulls from transcripts, retrieves relevant segments, and answers in natural language.  
- **Impact**: Makes learning, research, and entertainment more efficient by cutting through clutter and surfacing key insights instantly.  

---

## 🚀 Features  

- **Ask Anything**: Query specific questions related to the video and get clear, human-like answers which only refer to the video.  
- **Summarization**: Generate concise summaries of long lectures, podcasts, or tutorials.  
- **Smart Search**: Jump straight to the most relevant video segments instead of wasting time scrolling.  
- **Study Aid**: Perfect for students cramming before exams — get notes and key points instantly.  
- **Open Source**: Built with free and open-source AI models (LangChain + LLaMA 3 + FAISS).
- **Embedded Video**: For convinence, you can also watch video right there on the website.

---

## 🛠️ Tech Stack  
- **Streamlit** – UI  
- **LangChain** – orchestration (`Runnable*`, prompt templates)  
- **FAISS** – vector store  
- **SentenceTransformers** – embeddings (`all-MiniLM-L6-v2`)  
- **Ollama + LLaMA** – local LLM inference  
- **YouTube Transcript API** – transcript fetching

## Installation

### 1. Clone the repository
```bash
git clone https://github.com/your-username/talkwithtube.git
cd talkwithtube
```

### 2. Create and activate a virtual environment
```bash
python -m venv .venv
# Mac/Linux
source .venv/bin/activate
# Windows
# .venv\Scripts\activate
```

### 3. Install Python dependencies
If your repo includes a `requirements.txt`:
```bash
pip install -r requirements.txt
```

## 🦙 Set Up LLaMA Locally (via Ollama)

This app uses `ChatOllama` from `langchain_community`, which connects to a **local** Ollama server.

1) Install Ollama:  
https://ollama.ai/download

2) Pull the LLaMA model (example uses `llama3`):
```bash
ollama pull llama3
```

3) Quick test:
```bash
ollama run llama3 "Hello from LLaMA"
```

> If this prints a response, your local model is ready. Make sure Ollama is running in the background when you start the app!

## 🚀 Run the App
Start Streamlit (replace `app.py` with your script filename if different):
```bash
streamlit run transcript_chatbot.py
```

Open the URL shown in your terminal (usually `http://localhost:8501`).

## 📂 How to Use
1. In the left sidebar, choose **YouTube Video ID** and paste a full YouTube URL **(video must have subtitles)**, or upload a `.txt` transcript.  
2. The app will chunk + embed the transcript and build a FAISS index.  
3. Ask questions in the chat box.  
4. Answers are generated by your **local** LLaMA model using retrieved context.

## ❗ Notes & Tips
- Ensure the YouTube video **has captions**; otherwise the transcript cannot be fetched.  
- Performance (speed, context length) depends on your hardware and the chosen LLaMA model.  
- You can tweak retriever settings (e.g., `k=4`) in the code to return more/fewer chunks.hain-core faiss-cpu sentence-transformers youtube-transcript-api


---
